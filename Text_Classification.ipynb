{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n",
      "NLTK: 3.4\n",
      "Scikit-learn: 0.20.1\n",
      "Pandas: 0.23.4\n",
      "Numpy: 1.15.4\n"
     ]
    }
   ],
   "source": [
    "import sys #module allows us to operate on underlying interpreter (Python interpreter is a bytecode interpreter: its input is instruction sets called bytecode)\n",
    "import nltk #module allows to work with human language data\n",
    "import sklearn #module allows us to work with machine learning library for python\n",
    "import pandas #data analysis library whic provides easy-to-use data structures for data analysis.\n",
    "import numpy #library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Dataset of SMS Messages\n",
    "# read_table(...) - Read general delimited file into DataFrame\n",
    "df = pd.read_table(\"SMSSpamCollection\", header = None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "#Print Info about the Data Set\n",
    "print(df.info()) # full summary of the datafram.\n",
    "print(df.head()) # returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check Class distribution, to check how many spam or ham messages we have\n",
    "classes = df[0] #First column of the dataset\n",
    "print(classes.value_counts()) #resulting object will be in descending order so that the first element is the most frequently-occurring element. Excludes NA values by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "5    spam\n",
      "6     ham\n",
      "7     ham\n",
      "8    spam\n",
      "9    spam\n",
      "Name: 0, dtype: object\n",
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Convert class labels to classification values(binary values, 0=ham, 1=spam) since ham and spam doesnt make sense to machine learning algorithms.\n",
    "#we will use sklearn encoder to convert.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder() # used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "Y = encoder.fit_transform(classes) # Fit label encoder and return encoded labels\n",
    "print(classes[:10])\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "5    FreeMsg Hey there darling it's been 3 week's n...\n",
      "6    Even my brother is not like to speak with me. ...\n",
      "7    As per your request 'Melle Melle (Oru Minnamin...\n",
      "8    WINNER!! As a valued network customer you have...\n",
      "9    Had your mobile 11 months or more? U R entitle...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Store SMS Message Data\n",
    "text_messages = df[1]\n",
    "print(text_messages[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Regular expression to replace email address, urls, phone numbers, other numbers, symbols\n",
    "#replace email addresses with 'emailaddr'\n",
    "processed = text_messages.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress');\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress')\n",
    "\n",
    "# Replace money symbols with 'moneysymb' (£ can by typed with ALT key + 156)\n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "    \n",
    "# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr')\n",
    "    \n",
    "# Replace numbers with 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go until jurong point crazy available only in ...\n",
      "1                                 ok lar joking wif u oni\n",
      "2       free entry in numbr a wkly comp to win fa cup ...\n",
      "3             u dun say so early hor u c already then say\n",
      "4       nah i don t think he goes to usf he lives arou...\n",
      "5       freemsg hey there darling it s been numbr week...\n",
      "6       even my brother is not like to speak with me t...\n",
      "7       as per your request melle melle oru minnaminun...\n",
      "8       winner as a valued network customer you have b...\n",
      "9       had your mobile numbr months or more u r entit...\n",
      "10      i m gonna be home soon and i don t want to tal...\n",
      "11      six chances to win cash from numbr to numbr nu...\n",
      "12      urgent you have won a numbr week free membersh...\n",
      "13      i ve been searching for the right words to tha...\n",
      "14                      i have a date on sunday with will\n",
      "15      xxxmobilemovieclub to use your credit click th...\n",
      "16                                 oh k i m watching here\n",
      "17      eh u remember how numbr spell his name yes i d...\n",
      "18      fine if that s the way u feel that s the way i...\n",
      "19      england v macedonia dont miss the goals team n...\n",
      "20               is that seriously how you spell his name\n",
      "21      i m going to try for numbr months ha ha only j...\n",
      "22         so ü pay first lar then when is da stock comin\n",
      "23      aft i finish my lunch then i go str down lor a...\n",
      "24      ffffffffff alright no way i can meet up with y...\n",
      "25      just forced myself to eat a slice i m really n...\n",
      "26                          lol your always so convincing\n",
      "27      did you catch the bus are you frying an egg di...\n",
      "28      i m back amp we re packing the car now i ll le...\n",
      "29      ahhh work i vaguely remember that what does it...\n",
      "                              ...                        \n",
      "5542             armand says get your ass over to epsilon\n",
      "5543                u still havent got urself a jacket ah\n",
      "5544    i m taking derek amp taylor to walmart if i m ...\n",
      "5545        hi its in durban are you still on this number\n",
      "5546             ic there are a lotta childporn cars then\n",
      "5547    had your contract mobile numbr mnths latest mo...\n",
      "5548                     no i was trying it all weekend v\n",
      "5549    you know wot people wear t shirts jumpers hat ...\n",
      "5550            cool what time you think you can get here\n",
      "5551    wen did you get so spiritual and deep that s g...\n",
      "5552    have a safe trip to nigeria wish you happiness...\n",
      "5553                           hahaha use your brain dear\n",
      "5554    well keep in mind i ve only got enough gas for...\n",
      "5555    yeh indians was nice tho it did kane me off a ...\n",
      "5556    yes i have so that s why u texted pshew missin...\n",
      "5557    no i meant the calculation is the same that lt...\n",
      "5558                                sorry i ll call later\n",
      "5559    if you aren t here in the next lt gt hours imm...\n",
      "5560                      anything lor juz both of us lor\n",
      "5561    get me out of this dump heap my mom decided to...\n",
      "5562    ok lor sony ericsson salesman i ask shuhui the...\n",
      "5563                               ard numbr like dat lor\n",
      "5564    why don t you wait til at least wednesday to s...\n",
      "5565                                            huh y lei\n",
      "5566    reminder from onumbr to get numbr pounds free ...\n",
      "5567    this is the numbrnd time we have tried numbr c...\n",
      "5568                  will ü b going to esplanade fr home\n",
      "5569    pity was in mood for that so any other suggest...\n",
      "5570    the guy did some bitching but i acted like i d...\n",
      "5571                            rofl its true to its name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#change words to lower case - Hello, HELLO, hello are all same word\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus is a large collection of texts.\n",
    "#nltk.corpus : The modules in this package provide functions that can be used to read corpus files in a variety of formats\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Stopwords : Words in English which doesn't add any useful information to the message, \n",
    "#natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words\n",
    "#Set() creates an interable set.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#lambda operator or lambda function is used for creating small, one-time and anonymous function objects in Python\n",
    "#Append to the string all of teh words as long as they are not included in stop_words\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazi avail bugi n great world...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri numbr wkli comp win fa cup final tk...\n",
      "3                     u dun say earli hor u c alreadi say\n",
      "4                    nah think goe usf live around though\n",
      "5       freemsg hey darl numbr week word back like fun...\n",
      "6           even brother like speak treat like aid patent\n",
      "7       per request mell mell oru minnaminungint nurun...\n",
      "8       winner valu network custom select receivea mon...\n",
      "9       mobil numbr month u r entitl updat latest colo...\n",
      "10      gonna home soon want talk stuff anymor tonight...\n",
      "11      six chanc win cash numbr numbr numbr pound txt...\n",
      "12      urgent numbr week free membership moneysymbnum...\n",
      "13      search right word thank breather promi wont ta...\n",
      "14                                            date sunday\n",
      "15      xxxmobilemovieclub use credit click wap link n...\n",
      "16                                             oh k watch\n",
      "17      eh u rememb numbr spell name ye v naughti make...\n",
      "18                             fine way u feel way gota b\n",
      "19      england v macedonia dont miss goal team news t...\n",
      "20                                      seriou spell name\n",
      "21                          go tri numbr month ha ha joke\n",
      "22                         ü pay first lar da stock comin\n",
      "23      aft finish lunch go str lor ard numbr smth lor...\n",
      "24                     ffffffffff alright way meet sooner\n",
      "25      forc eat slice realli hungri tho suck mark get...\n",
      "26                                      lol alway convinc\n",
      "27      catch bu fri egg make tea eat mom left dinner ...\n",
      "28                        back amp pack car let know room\n",
      "29                    ahhh work vagu rememb feel like lol\n",
      "                              ...                        \n",
      "5542                           armand say get ass epsilon\n",
      "5543                  u still havent got urself jacket ah\n",
      "5544    take derek amp taylor walmart back time done l...\n",
      "5545                               hi durban still number\n",
      "5546                               ic lotta childporn car\n",
      "5547    contract mobil numbr mnth latest motorola noki...\n",
      "5548                                        tri weekend v\n",
      "5549    know wot peopl wear shirt jumper hat belt know...\n",
      "5550                                  cool time think get\n",
      "5551                           wen get spiritu deep great\n",
      "5552    safe trip nigeria wish happi soon compani shar...\n",
      "5553                                hahaha use brain dear\n",
      "5554    well keep mind got enough ga one round trip ba...\n",
      "5555    yeh indian nice tho kane bit shud go numbr dri...\n",
      "5556                            ye u text pshew miss much\n",
      "5557    meant calcul lt gt unit lt gt school realli ex...\n",
      "5558                                     sorri call later\n",
      "5559                       next lt gt hour imma flip shit\n",
      "5560                                 anyth lor juz us lor\n",
      "5561                get dump heap mom decid come low bore\n",
      "5562    ok lor soni ericsson salesman ask shuhui say q...\n",
      "5563                               ard numbr like dat lor\n",
      "5564                     wait til least wednesday see get\n",
      "5565                                              huh lei\n",
      "5566    remind onumbr get numbr pound free call credit...\n",
      "5567    numbrnd time tri numbr contact u u moneysymbnu...\n",
      "5568                              ü b go esplanad fr home\n",
      "5569                                    piti mood suggest\n",
      "5570    guy bitch act like interest buy someth el next...\n",
      "5571                                       rofl true name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Stem the word, extract the words,\n",
    "#A stem is a part of a word, a stem is a form to which affixes can be attached\n",
    "#For Ex : English word friendships contains the stem friend, to which the derivational suffix -ship is attached to form a new stem friendship, to which the inflectional suffix -s is attached.\n",
    "#remove word stems using a porter stemmer(Algorithm or process for removing the commoner morphological(relating to form) and inflexional( involving a change in the form of a word) endings from words in English)\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "#stem(term) : Strip affixes from the term and return the stem.\n",
    "processed = processed.apply(lambda x:' '.join(porter_stemmer.stem(term) for term in x.split()))\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to generating features.\n",
    "# Feature Engineering is the process of using domain knowledge of data which we have gained by going through these data, \n",
    "# to create features for machine learning algorithms\n",
    "# so here in that date the words in the text message is going to be our features and for this purpose it is neccessary to tokenize each word\n",
    "from nltk.tokenize import word_tokenize #A tokenizer that divides a string into substrings by splitting on the specified string (defined in subclasses).\n",
    "\n",
    "#Extracting all the words as tokens bag of words model\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "\n",
    "# FreqDist() class is used to encode “frequency distributions”, which count the number of times that each outcome of an experiment occurs.\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 10097\n",
      "most common words: [('numbr', 2642), ('I', 2022), ('u', 821), ('call', 405), ('U', 386), ('get', 338), ('gt', 318), ('lt', 316), ('moneysymbnumbr', 303), ('ur', 298), ('You', 290), ('know', 255), ('go', 250), ('like', 235), ('got', 217)]\n"
     ]
    }
   ],
   "source": [
    "# Print total number of words and also 15 most common words\n",
    "print('number of words: {}'.format(len(all_words)))\n",
    "print('most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2000 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go\n",
      "jurong\n",
      "point\n",
      "crazy\n",
      "Available\n",
      "bugis\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "Cine\n",
      "got\n",
      "amore\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "# Define a function that will determine which of these 2000 word features are contained in each review \n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features\n",
    "\n",
    "# Example\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Features for all the messages\n",
    "messages = list(zip(processed, Y)) # Note : Y is spam or not spam, binary class label\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and testing data sets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 4179\n",
      "Testing: 1393\n"
     ]
    }
   ],
   "source": [
    "# Print length of training and testing the data\n",
    "print('Training: {}'.format(len(training)))\n",
    "print('Testing: {}'.format(len(testing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('K Nearest Neighbors', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('Decision Tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')), ('Random Forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)), ('Logistic Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)), ('SGD Classifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=100,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)), ('Naive Bayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)), ('SVM Linear', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "# Define models to train\n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression', 'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors: Accuracy: 93.75448671931083\n",
      "Decision Tree: Accuracy: 97.5592246949031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheta\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: Accuracy: 98.42067480258436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Accuracy: 98.77961234745155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheta\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier: Accuracy: 97.91816223977028\n",
      "Naive Bayes: Accuracy: 98.42067480258436\n",
      "SVM Linear: Accuracy: 98.34888729361091\n"
     ]
    }
   ],
   "source": [
    "# wrap models in NLTK \n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing) * 100\n",
    "    print('{}: Accuracy: {}'.format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 98.34888729361091\n"
     ]
    }
   ],
   "source": [
    "# Ensemble methods for Voting classifier \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1192\n",
      "           1       0.99      0.94      0.96       201\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1393\n",
      "   macro avg       0.99      0.97      0.98      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1190</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>13</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1190    2\n",
       "       spam        13  188"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
